# 🚀 Lightweight Transformer for Resource-Constrained Environments

This repository contains my internship project report focused on **designing and optimizing lightweight Transformer architectures** for resource-constrained environments (e.g., edge devices, embedded systems, and low-memory setups).

## 📄 About the Report

The internship project is **still in progress**, and the current report primarily covers:

- ✅ **Study of Transformer models** – their architecture, working principles, and evolution  
- ✅ **Analysis of various Transformer variants** – including Vision Transformers (ViT), MobileBERT, DistilBERT, and other lightweight approaches  
- ✅ **Optimization techniques** – methodologies to minimize **computational complexity** (FLOPs) and **memory usage** while retaining performance  
- ✅ **Future directions** – a survey of state-of-the-art research papers and experimental ideas for further optimization  

You can find the report here:  
👉 [📄 Internship Project Report (PDF)](./Internship_Project_Report.pdf)

---

## 🛠️ Project Status

🔄 **Work In Progress (WIP)**  
The current phase focuses on **literature review & methodology exploration**.  
Upcoming phases will include:  
- Implementing selected optimization techniques  
- Benchmarking and performance evaluation  
- Deployment on low-resource environments  

---

## 📚 Topics Covered

- Transformers and Self-Attention  
- Model Compression (Pruning, Quantization, Knowledge Distillation)  
- Efficient Attention Mechanisms (Linformer, Performer, Reformer, etc.)  
- Parameter Sharing & Low-Rank Factorization  
- Future research directions for lightweight deep learning  

---

## 🔗 References

The report is based on insights from:  
- **Key research papers** on Transformer optimization  
- **Experimentation results** and performance benchmarks from open-source implementations  
- **Surveys and reviews** in the field of efficient deep learning  

---

## 📬 Contact

For any queries or discussion, feel free to reach out:  
**Rounak Sahas**  
📧 *[Your Email Here]*  

---

> *Note: This repository will be updated with implementation details and further results as the internship progresses.*

